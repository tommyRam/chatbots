{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "878a0ad8-5a96-4e6c-b00c-c71b2d090102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "952a9ebe-5323-4fc1-ad41-2cc6e9f13665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "doc_path = \"dev-data/Be_Good.pdf\"\n",
    "loader = PyPDFLoader(doc_path)\n",
    "\n",
    "doc = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "doc_splits = text_splitter.split_documents(doc)\n",
    "\n",
    "chromadb = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    ")\n",
    "retriever = chromadb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2de17599-f90a-43e4-99da-9baef48af82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input \n",
    "question.\\n The goal is to break down the input into a set of sub-problems / sub-questions that can be \n",
    "answers in isolation. (Directly return the queries, don't add a phrase before the queries)\\n Generate multiple search queries related to : {question}\\n Ouput (3 queries):\n",
    "\"\"\"\n",
    "\n",
    "prompt_decompositions = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash-preview-05-20\", temperature=0)\n",
    "\n",
    "sub_question_generator_chain = (\n",
    "    prompt_decompositions \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "question = \"What is the name of the writter of the book and what the book says about ?\"\n",
    "questions = sub_question_generator_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63aab210-e6de-4cd3-adb6-05cc0fd52f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Book Name] author', '[Book Name] summary', '[Book Name] main themes']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a5e0453-a620-4675-aa1b-df7f237fb329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy RAMAROKOTO\\Documents\\programmation\\projects\\New folder\\chatbots\\backend\\.venv\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\": question})\n",
    "    rag_results = []\n",
    "\n",
    "    for sub_question in sub_questions:\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs,\n",
    "                                                               \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "\n",
    "    return rag_results, sub_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d056fdf-01ee-4640-b30e-91484f0a7b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy RAMAROKOTO\\AppData\\Local\\Temp\\ipykernel_2772\\3895697035.py:15: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(sub_question)\n"
     ]
    }
   ],
   "source": [
    "answers, questions = retrieve_and_rag(question, prompt_rag, sub_question_generator_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1193bdf5-c2da-436a-95c3-c41b364daf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The writer is Paul Graham. The work, titled \"Be Good - Essay by Paul Graham,\" discusses concepts like benevolence and \"being good\" as a strategy. Its main themes revolve around the inherent value and advantages of benevolence, suggesting that doing good provides a sense of mission, encourages others to offer help, and that avoiding evil can lead to long-term success.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c20d81-a798-43e6-bf7b-b65f48cf231f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
