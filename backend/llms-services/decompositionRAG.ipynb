{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20d37bf9-7d93-4bb8-8998-8e8275d066f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da9d36fd-df73-4389-a84a-05c52c431d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input \n",
    "question.\\n The goal is to break down the input into a set of sub-problems / sub-questions that can be \n",
    "answers in isolation.\\n Generate multiple search queries related to : {question}\\n Ouput (3 queries):\n",
    "\"\"\"\n",
    "\n",
    "prompt_decompositions = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c9ce1ac-0a46-4ae6-b0aa-e1537c61773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash-preview-05-20\")\n",
    "\n",
    "generate_queries_decompositions_chain = (\n",
    "    prompt_decompositions\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "question = \"What is the name of the writter of the book and what the book says about ?\"\n",
    "questions = generate_queries_decompositions_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69f33dae-e9b7-408b-b118-bc4c8c825c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are three search queries related to your question, assuming you will replace `[Book Title]` with the actual name of the book you are interested in:',\n",
       " '',\n",
       " '1.  `[Book Title] author`',\n",
       " '2.  `[Book Title] summary`',\n",
       " '3.  `who wrote [Book Title] and what is it about`']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0fa2b05-1276-46ef-b9a8-acc3e83fa526",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_template = \"\"\"Here is the question you need to answer:\n",
    "\\n-----\\n {question} \\n-----\\n\n",
    "\n",
    "Here is any available background question + answer pairs: \n",
    "\n",
    "\\n-----\\n {q_a_pairs} \\n-------\\n\n",
    "\n",
    "Here is additional context relevant to the questions:\n",
    "\n",
    "\\n-----\\n {context} \\n--------\\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\"\"\"\n",
    "\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(decomposition_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "557b140f-83d5-49fb-bac4-a32e96186e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a4a213-651c-421e-8304-06d180e28e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "docs_path = \"dev-data/Be_Good.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(docs_path)\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8893090-183c-4710-8f8f-ba62fa20c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash-preview-05-20\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "questions = questions[2:]\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm \\\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n----\\n\" + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c907a5d2-1bf5-4ec5-ab40-0bed8a93cc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The essay \"Be Good\" was written by Paul Graham.\\n\\nIn this essay, Paul Graham argues that being \"good\" is not merely a statement of values but a practical and effective strategy. He suggests that this approach can serve as a guide for strategy and even a design specification for software, emphasizing that it \"works.\" Graham encourages going beyond simply \"not being evil\" to actively \"being good,\" citing examples like Craigslist as successful entities that operate with a \"good\" ethos.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0981e4e-8e4d-4428-afb2-6c7951dfa785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
