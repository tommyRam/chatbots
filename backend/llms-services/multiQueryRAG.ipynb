{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c4e398-5a3c-41d1-ad1a-f014fd0cecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43164931-ba37-4bf5-b406-c53f750c4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Indexing \n",
    "import bs4\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "doc_path = \"dev-data/Be_Good.pdf\"\n",
    "loader = PyPDFLoader(doc_path)\n",
    "\n",
    "doc = loader.load()\n",
    "\n",
    "# Split \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(doc)\n",
    "\n",
    "# Index\n",
    "chromadb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    ")\n",
    "retriever = chromadb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5be58-91df-4e7c-8867-5a8564d7b2f0",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08403239-725b-4a24-96fe-48327bfac08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Mult query: different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five different\n",
    "versions of te given user question to retrieve relevant documents from a vector database.\n",
    "By generating multiple perpectives on the user question, your goal is to help the user overcome\n",
    "some of the limitations of the distance-based similarity search. Provide these alternative questions\n",
    "separated by newlines. Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries_chain = (\n",
    "    prompt_perspectives\n",
    "    | ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash-preview-05-20\")\n",
    "    | StrOutputParser()\n",
    "    | (lambda x : x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8242ce6-d7f6-40aa-98b4-541b9416d007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who authored the book?',\n",
       " \"What is the name of the book's author?\",\n",
       " 'By whom was this publication penned?',\n",
       " 'Can you identify the individual who wrote the book?',\n",
       " 'Who is credited with writing the book?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_chain.invoke(\"Who is the writter of the book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "004bba15-00df-4feb-b27f-7e1f88db73fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy RAMAROKOTO\\AppData\\Local\\Temp\\ipykernel_8156\\3792943944.py:7: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\"Unique union of retrieved docs\"\"\"\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "question = \"Who is the writer of the book?\"\n",
    "retrieval_chain = generate_queries_chain | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e60b211-778d-4709-98e3-bb86239bebf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The writer of the essay \"Be Good\" is Paul Graham.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the followng question based on this context\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash-preview-05-20\")\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")}\n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d9e53-6126-4c17-bc42-115d33204a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
