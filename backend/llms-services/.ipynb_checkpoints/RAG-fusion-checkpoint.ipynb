{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c029c2e-3bd2-4a80-a010-d2a4de00dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def7e8ce-0024-45ed-89ff-47dc68337153",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Indexing \n",
    "import bs4\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "doc_path = \"dev-data/Be_Good.pdf\"\n",
    "loader = PyPDFLoader(doc_path)\n",
    "\n",
    "doc = loader.load()\n",
    "\n",
    "# Split \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(doc)\n",
    "\n",
    "# Index\n",
    "chromadb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    ")\n",
    "retriever = chromadb.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c55b4ff-06ce-41db-9091-7240d14ce459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on\n",
    "single input query.\\nGenerate multiple search queries related to: {question}\\n Output \n",
    "(4 queries):\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b7a4569-1952-45f3-aaa2-fbdbbf9ccc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e99280c-6014-4aa2-8f2c-3fbaabbce688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are 4 multiple search queries related to \"Who is the writer of the book?\":',\n",
       " '',\n",
       " '1. **Author of the book**: Who wrote this book?',\n",
       " '2. **Book author search**: What is the name of the author of the book?',\n",
       " '3. **Writer of the book**: Who is the creator of this book?',\n",
       " '4. **Book writer query**: Can you identify the author of this book?',\n",
       " '',\n",
       " 'Let me know if you need more!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who is the writter of the book?\"\n",
    "generate_queries.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6473047c-93a4-4b73-9d83-55c5dbf971fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy RAMAROKOTO\\AppData\\Local\\Temp\\ipykernel_7184\\2401826664.py:16: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  (loads(doc), score)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads \n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
    "        and an optioan parameter k used in the RRF formula\"\"\"\n",
    "    fused_scores = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "449ade9f-3170-4aee-8219-c0ccbd8e0608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The writer of the book is Paul Graham.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-flash-preview-05-20\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c92a1-ca86-451f-b180-c2d4489180de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
